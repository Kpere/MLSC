# -*- coding: utf-8 -*-
"""MLSC Model Architecture.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-97O1IdBrQtJGxMiV5OqWs8bVsfUGFJJ
"""

# Import necessary libraries
import tensorflow as tf
from tensorflow.keras.layers import (Input, Conv1D, MaxPooling1D, LSTM, GRU, TimeDistributed,
                                     Dense, MultiHeadAttention, LayerNormalization, Lambda,
                                     Flatten, Concatenate, Embedding, Reshape)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

def build_model(X_train_reshaped, y_train):
    # Define input tensors for each label column
    input_label_1d = Input(shape=(1,), name='input_label_1d')
    input_label_3d = Input(shape=(1,), name='input_label_3d')
    input_label_7d = Input(shape=(1,), name='input_label_7d')
    input_label_10d = Input(shape=(1,), name='input_label_10d')

    # Define Embedding layers for each label column
    label_1d_embedding = Embedding(input_dim=2, output_dim=2)(input_label_1d)
    label_3d_embedding = Embedding(input_dim=2, output_dim=2)(input_label_3d)
    label_7d_embedding = Embedding(input_dim=2, output_dim=2)(input_label_7d)
    label_10d_embedding = Embedding(input_dim=2, output_dim=2)(input_label_10d)

    # Define model architecture
    input_layer = Input(shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), name="input_layer")
    cnn_layer = Conv1D(32, kernel_size=3, activation='relu')(input_layer)
    cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)
    lstm_layer = GRU(64, return_sequences=True)(cnn_layer)
    timevec_layer = TimeDistributed(Dense(1, activation='relu'))(lstm_layer)
    attention_layer = MultiHeadAttention(num_heads=2, key_dim=16)(query=lstm_layer, value=timevec_layer)
    lstm_attention_layer = Lambda(lambda x: x[0] + x[1])([lstm_layer, attention_layer])
    lstm_attention_layer = LayerNormalization()(lstm_attention_layer)
    dense_layer = Dense(32, activation='relu')(lstm_attention_layer)
    flatten_layer = Flatten()(dense_layer)

    # Flatten the embedding layers
    label_1d_embedding_flat = Flatten()(label_1d_embedding)
    label_3d_embedding_flat = Flatten()(label_3d_embedding)
    label_7d_embedding_flat = Flatten()(label_7d_embedding)
    label_10d_embedding_flat = Flatten()(label_10d_embedding)

    # Concatenate embedding layers with flatten_layer
    concatenated_layers = Concatenate()([flatten_layer, label_1d_embedding_flat, label_3d_embedding_flat, label_7d_embedding_flat, label_10d_embedding_flat])

    # Output layer with sigmoid activation for multi-label classification
    output_layer = Dense(4, activation='sigmoid', name="output_layer")(concatenated_layers)

    # Create the model
    model = tf.keras.Model(inputs=[input_layer, input_label_1d, input_label_3d, input_label_7d, input_label_10d], outputs=output_layer, name="custom_transformer_model")

    # Compile the model
    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])

    return model