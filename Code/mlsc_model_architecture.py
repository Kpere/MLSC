# -*- coding: utf-8 -*-
"""MLSC Model Architecture.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-97O1IdBrQtJGxMiV5OqWs8bVsfUGFJJ
"""

# Import necessary libraries
import tensorflow as tf
from tensorflow.keras.layers import (Input, Conv1D, MaxPooling1D, LSTM, GRU, TimeDistributed,
                                     Dense, MultiHeadAttention, LayerNormalization, Lambda,
                                     Flatten, Concatenate, Embedding, Reshape)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

def build_model(X_train_reshaped):
    # Define input tensors for each label column
    input_label_1d = Input(shape=(1,), name='input_label_1d')
    input_label_3d = Input(shape=(1,), name='input_label_3d')
    input_label_7d = Input(shape=(1,), name='input_label_7d')
    input_label_10d = Input(shape=(1,), name='input_label_10d')

    # Define Embedding layers for each label column
    label_1d_embedding = Embedding(input_dim=2, output_dim=2)(input_label_1d)
    label_3d_embedding = Embedding(input_dim=2, output_dim=2)(input_label_3d)
    label_7d_embedding = Embedding(input_dim=2, output_dim=2)(input_label_7d)
    label_10d_embedding = Embedding(input_dim=2, output_dim=2)(input_label_10d)

    # Define model architecture
    input_layer = Input(shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), name="input_layer")
    cnn_layer = Conv1D(128, kernel_size=3, activation='relu')(input_layer)
    cnn_layer = GlobalMaxPooling1D(pool_size=2)(cnn_layer)
    gru_layer = GRU(64, return_sequences=True)(cnn_layer)
    timevec_layer = TimeDistributed(Dense(1, activation='relu'))(gru_layer)
    attention_layer = MultiHeadAttention(num_heads=2, key_dim=64)(query=gru_layer, value=timevec_layer)
    gru_attention_layer = Lambda(lambda x: x[0] + x[1])([gru_layer, attention_layer])
    gru_attention_layer = LayerNormalization()(gru_attention_layer)
    dense_layer = Dense(32, activation='relu')(gru_attention_layer)
    flatten_layer = Flatten()(dense_layer)

    # Flatten the embedding layers
    label_1d_embedding_flat = Flatten()(label_1d_embedding)
    label_3d_embedding_flat = Flatten()(label_3d_embedding)
    label_7d_embedding_flat = Flatten()(label_7d_embedding)
    label_10d_embedding_flat = Flatten()(label_10d_embedding)

    # Concatenate embedding layers with flatten_layer
    concatenated_layers = Concatenate()([flatten_layer, label_1d_embedding_flat, label_3d_embedding_flat, label_7d_embedding_flat, label_10d_embedding_flat])

    # Residual connection
    residual_output = Add()([concatenated_layers, gru_attention_layer])

    # Output layers for multi-label classification
    output_layers = []
    for i in range(4):
        output_layer = Dense(3, activation='softmax', name=f"output_{i}")(residual_output)
        output_layers.append(output_layer)

    # Create the model
    model = Model(inputs=[input_layer, input_label_1d, input_label_3d, input_label_7d, input_label_10d], outputs=output_layers, name="custom_transformer_model")

    # Compile the model
    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    return model

def train_model(model, X_train, y_train, epochs=50, batch_size=16, validation_split=0.2):
    # Early stopping callback
    early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)
    # Train the model
    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,
                        validation_split=validation_split, callbacks=[early_stopping])
    return history